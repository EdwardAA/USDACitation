{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c881239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96818658",
   "metadata": {},
   "source": [
    "## Upload PDF files from extension reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db991c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REF_text=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dea0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uppload sample pdf \"00_2.pdf\"\n",
    "# just need to put the pdf in the wroking directory and refer to the name in teh line below, i.e. \"00_2.pdf\"\n",
    "# ths process can be repeated where there are more than one file \n",
    "reader = PdfReader(\"00_2.pdf\", 'wb')\n",
    "no_pages=len(reader.pages)\n",
    "text=\"\"\n",
    "for j in range(no_pages): #no_pages-1\n",
    "    page=reader.pages[j]\n",
    "    text+=page.extract_text()\n",
    "REF_text.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7add8",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08000467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20b1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(TEXT):\n",
    "    new_stopwords=[\"html\",\"url\", \"endif\", \"doctyp\", \"endif\", \"htmlrdfa\",\"style\",\"import\", \"minwidth\", \"script\" ]\n",
    "    ####################################################################################################################\n",
    "    if type(TEXT)==str:\n",
    "        all_reviews = list()\n",
    "        text=TEXT        \n",
    "        text = text.lower()\n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "#         text = re.sub(r\"\\W\", \"\", text)\n",
    "#         text = re.sub(r\"\\s+\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.update(new_stopwords)\n",
    "        stop_words.discard(\"not\")\n",
    "        PS = PorterStemmer()\n",
    "        words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        all_reviews.append(words)       \n",
    "    ####################################################################################################################    \n",
    "    else:\n",
    "        all_reviews = list()\n",
    "        for text in TEXT:\n",
    "            text = text.lower()\n",
    "            pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "            text = pattern.sub('', text)\n",
    "            text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "    #         text = re.sub(r\"\\W\", \"\", text)\n",
    "    #         text = re.sub(r\"\\s+\", \"\", text)\n",
    "            tokens = word_tokenize(text)\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in tokens]\n",
    "            words = [word for word in stripped if word.isalpha()]\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            stop_words.update(new_stopwords)\n",
    "            stop_words.discard(\"not\")\n",
    "            PS = PorterStemmer()\n",
    "            words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "            words = ' '.join(words)\n",
    "            all_reviews.append(words)\n",
    "    ####################################################################################################################\n",
    "    return all_reviews\n",
    "\n",
    "TEST=clean_text(REF_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "024a8b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['market livestock weight live recommend ideal weight rang accept weight rang unaccept weight low high low high market beef steer market beef heifer market dairi steer beef feeder steer dairi beef feeder steer market lamb market goat market hog per recommend anim scienc strateg advisori committe minimum accept weight depend upon opportuni ty youth extend project onto market beef dairi anim would unaccept mani locat market rabbit weight live recommend ideal weight rang accept weight rang unaccept weight low high min max rabbit fryer wk old not wk rabbit roaster must mo rabbit stewer must mo age upper limit upper limit per arba rabbit standard standard perfect market anim weight recommend per recommend anim scienc strateg advisori committe novemb market livestock weight carcass recommend ideal weight rang accept weight rang unaccept weight low high low high market beef steer heifer market dairi steer market lamb market goat market hog']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ff90d",
   "metadata": {},
   "source": [
    "## Save results as csv (to be read by the main/prediction code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb0655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "TEST_EXT=pd.DataFrame()\n",
    "TEST_EXT['content']=TEST\n",
    "\n",
    "TEST_EXT.to_csv('TEST_EXT.csv', index=False)\n",
    "\n",
    "# pd2=pd.read_csv('TEST_EXT.csv')\n",
    "# pd2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19075a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf5",
   "language": "python",
   "name": "tf5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
